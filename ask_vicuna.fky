#!/usr/bin/env fkyrun

  Copyright (C) 2024 by
  Dipl.-Ing. Michael Niederle

  This program is free software; you can redistribute it and/or modify
  it under the terms of the GNU Library General Public License, version 2, or
  (at your option) under the terms of the GNU Lesser General Public License,
  version 3.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
  GNU Lesser (Library) General Public License for more details.

  For details of the GNU General Public License see the accompanying
  files LGPLv2.txt and LGLPv3.txt or
  http://www.gnu.org/licenses/lgpl-2.0.html
  http://www.gnu.org/licenses/lgpl-3.0.html
  or print to the
  Free Software Foundation, Inc.,
  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.

<require basic/stdlib>
<require basic/uuid>
<require basic/export/json>
<require basic/import/json>
<require terminal/terminal>
<require ai/llama>

<using std>
<using ai>

<allow unused>

program_parameters!
  $log_level
  list
    VALUED_OPTION "log" 0 to_integer "
      specify the log level (0 = disable)
  $verbose
  list
    "verbose" "
      display the text during generation
  $use_colours
  list
    "colour" "
      use background colours to display the confidence level
  $minimum_confidence
  list
    VALUED_OPTION "min" 15 to_number "
      the minimum confidence value
  $maximum_length
  list
    VALUED_OPTION "length" 1024 to_integer "
      the maximum number of tokens to generate
  $backtrack
  list
    VALUED_OPTION "backtrack" 0 to_integer "
      the maximum number of steps to backtrack
  $stop
  list
    VALUED_OPTION "stop" "
      a substring on which to stop the text generation
  $detailed
  list
    "detailed" "
      request a more detailed answer
  $german
  list
    "german" "
      use a german prompt text
  $model_name
  list
    VALUED_OPTION "model" "vicuna-13b-v1.5.Q4_K_M.gguf" "
      the model to use
  $do_start_server
  list
    "start-server" "
      start the ai server
  #$server
  #list
    VALUED_OPTION "server" "127.0.0.1:8000" "
      the ai server to use
  $server_name
  list
    VALUED_OPTION "server-name" "funky_server" "
      the name of the server's executable file
  $server_log_file
  list
    VALUED_OPTION "server-log" "/tmp/funky_server.log" "
      the log file of the ai server
  $model_path
  list
    VALUED_OPTION "model-path" "/var/models" "
      the directory where the model is located
  $question
  list
    OPTIONAL_PARAMETER "question" "
      the question you want to ask

$do_log log_level > 0

get_uuid! $uuid

on do_log:
  eprint! "
    model: @(model_name)
    minimum confidence: @(minimum_confidence)
    maximum length: @(maximum_length)
    backtrack: @(backtrack)
    detailed: @(if(detailed (-> "true") (-> "false")))
    uuid: @(uuid)
    log level: @(log_level)
    @;

$DETAILED
  if
    detailed
    -> " detailed"
    -> ""

$DETAILIERT
  if
    detailed
    -> " in ausfÃ¼hrlicher Weise"
    -> ""

update_if stop.is_defined &stop -> replace_all(stop "\n" = "@nl;")

load_ai_model! $vicuna
  UUID = uuid
  model_name

if
  vicuna.is_an_error:
    if
      do_start_server:
	getcwd! $working_directory
	$cmd append(working_directory "/" server_name)
	stat! $info cmd
	if
	  info.is_an_error:
	    which! !cmd server_name
	    start_server!
	  start_server

	$start_server:
	  if
	    cmd.is_defined:
	      on do_log: eprint! "
		starting ai-server "@(cmd)"
	      open! $fd_null "/dev/null" "r"
	      open! $fd_log server_log_file "w+"
	      create_process! $pid
		cmd
		list
		  "--uuid" uuid
		  "--model-path" model_path
		undefined
		fd_null fd_log fd_log
	      close! fd_null
	      close! fd_log
	      if
		pid.is_an_error:
		  Error "
		    failed to start ai-server: @(pid.to_error_message_string)
		:
		  on do_log: eprint! "
		    started ai-server with pid @(pid.to_integer)
		  start!
	    :
	      Error "
		could not find the ai-server executable "@(server_name)"
      :
	Error! vicuna.to_error_message_string
  start

$start:
  if
    question.is_undefined:
      update_if not(verbose || use_colours) &verbose -> true
      if
	german:
	  print! "
	    Stellen sie Fragen oder geben Sie Anweisungen.

	    Die KI merkt sich weder vorangegangen Fragen noch Antworten!

	    Geben sie "quit" ein, um die Konversation zu beenden.
	:
	  print! "
	    Ask questions or give instructions.

	    The AI will not remember your preceding questions and its answers.

	    Enter "quit" to stop the conversation.
	    @;
      loop:
	print! "> "
	getln! !question
	trim &question
	case question
	  "quit", "exit" cleanup
	  :
	    generate_response!
	    next!
    :
      generate_response!
      cleanup!

$generate_response:
  if
    vicuna.is_an_error:
      load_ai_model! !vicuna
	UUID = uuid
	model_name
      on vicuna.is_an_error: Error! vicuna.to_error_message_string
      send_request!
    send_request

  $send_request:
    generate! vicuna $response
      MINIMUM_CONFIDENCE = minimum_confidence
      MAXIMUM_LENGTH = maximum_length
      BACKTRACK = backtrack
      STOP = stop
      LOG_LEVEL = log_level
      VERBOSE = verbose
      USE_COLOURS = use_colours
      if
	german
	->
	  "
	    Beantworte die nachstehende Frage@(DETAILIERT) auf deutsch.
	    ### Frage: @(question)
	    ### Antwort:@
	->
	  "
	    Below is an instruction that describes a task, paired with an @
	    input that provides further context. Write a@(DETAILED) response @
	    that appropriately completes the request.
	    ### Instruction: @(question)
	    ### Response:@
    on not(verbose || use_colours): println! response

$cleanup:
  deregister! uuid
