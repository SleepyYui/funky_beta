#!/usr/bin/env fkyrun

  Copyright (C) 2023 by
  Dipl.-Ing. Michael Niederle

  This program is free software; you can redistribute it and/or modify
  it under the terms of the GNU Library General Public License, version 2, or
  (at your option) under the terms of the GNU Lesser General Public License,
  version 3.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
  GNU Lesser (Library) General Public License for more details.

  For details of the GNU General Public License see the accompanying
  files LGPLv2.txt and LGLPv3.txt or
  http://www.gnu.org/licenses/lgpl-2.0.html
  http://www.gnu.org/licenses/lgpl-3.0.html
  or print to the
  Free Software Foundation, Inc.,
  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.

<require basic/stdlib>
<require basic/export/json>
<require basic/import/json>
<require terminal/terminal>
<require ./ai/llama>

<using std>
<using ai>

<allow unused>

program_parameters!
  $log_level
  list
    VALUED_OPTION "log" 0 to_integer "
      specify the log level (0 = disable)
  $verbose
  list
    "verbose" "
      display the text during generation
  $use_colours
  list
    "colour" "
      use background colours to display the confidence level
  $minimum_confidence
  list
    VALUED_OPTION "min" 15 to_number "
      the minimum confidence value
  $maximum_length
  list
    VALUED_OPTION "length" 1024 to_integer "
      the maximum number of tokens to generate
  $backtrack
  list
    VALUED_OPTION "backtrack" 0 to_integer "
      the maximum number of steps to backtrack
  $stop
  list
    VALUED_OPTION "stop" "
      a text on which to stop generation
  $model_name
  list
    VALUED_OPTION "model" "vicuna-13b-v1.5.Q4_K_M.gguf" "
      the model to use
  $filename
  list
    MANDATORY_PARAMETER "filename" "
      the name of the file containing the prompt text

$do_log log_level > 0

on do_log:
  eprint! "
    model: @(model_name)
    prompt file: @(filename)
    minimum confidence: @(minimum_confidence)
    maximum length: @(maximum_length)
    backtrack: @(backtrack)
    stop: @(default_value(stop ""))
    log level: @(log_level)
    @;

load! $prompt filename

load_ai_model! $model model_name

generate! model $response
  MINIMUM_CONFIDENCE = minimum_confidence
  MAXIMUM_LENGTH = maximum_length
  BACKTRACK = backtrack
  STOP = stop
  LOG_LEVEL = log_level
  VERBOSE = verbose
  USE_COLOURS = use_colours
  prompt

println! response
