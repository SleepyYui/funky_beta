#
  Copyright (C) 2023 by
  Dipl.-Ing. Michael Niederle

  This program is free software; you can redistribute it and/or modify
  it under the terms of the GNU Library General Public License, version 2, or
  (at your option) under the terms of the GNU Lesser General Public License,
  version 3.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
  GNU Lesser (Library) General Public License for more details.

  For details of the GNU General Public License see the accompanying
  files LGPLv2.txt and LGLPv3.txt or
  http://www.gnu.org/licenses/lgpl-2.0.html
  http://www.gnu.org/licenses/lgpl-3.0.html
  or print to the
  Free Software Foundation, Inc.,
  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.

<namespace ai>
<namespace ai_types>

#
  Topic: AI

  Funky currently supports large language models based on the llama.cpp project.

# Attributes

$ai::pieces_of ()
  #
    a list of pieces (token texts)

$ai::piece_table_of ()
  #
    a hash table that maps all valid pieces to tokens

$ai::maximum_piece_length_of ()
  #
    the length of the longest piece (token string)

$ai::begin_of_stream_token_of ()
  #
    the token no. used to denote the begin of a stream

$ai::end_of_stream_token_of ()
  #
    the token no. used to denote the end of a stream

$ai::newline_token_of ()
  #
    the token no. used to denote a newline

$ai::end_of_text_token_of ()
  #
    the token no. used to denote the end of a text

$ai::prefix_token_of ()
  #
    the token no. used to denote a prefix

$ai::suffix_token_of ()
  #
    the token no. used to denote a suffix

$ai::middle_token_of ()
  #
    the token no. used to denote the "middle"

$ai::question_token_of ()

$ai::answer_token_of ()

$ai::context_size_of ()
  #
    the size of the context (number of tokens)

$std::address_of ()
  #
    the (IP) address of the server

$std::port_no_of ()
  #
    the port no. of the server

# Methods

$ai::evaluate ()
  #
    evaluate a prompt string or list of tokens

$ai::tokenize ()
  #
    convert a string into a list of tokens

$ai::detokenize ()
  #
    convert a single token or a list of tokens into a string

$ai::generate ()
  #
    generate a response for the specified prompt

# Options

$ai::MINIMUM_CONFIDENCE .
  #
    the minimum confidence level to continue generation

$ai::MAXIMUM_LENGTH .
  #
    the maximum number of tokens to generate

$ai::STOP .
  #
    a stop text

$ai::BACKTRACK .
  #
    specfies the maximum number of backtracking steps

$ai::LOG_LEVEL .
  #
    specifies the log level

$ai::VERBOSE .
  #
    if set the response text will be displayed during generation

$ai::USE_COLOURS .
  #
    use colours to display the response text

# Prototype Objects

$ai_types::model std_types::object
  #
    the base object for all AI models

# Implementation

$ai::load_ai_model:
  #
    load an AI model

    Topic: AI
  (
    name # the name of the model
    server = "127.0.0.1:8080"
  )
  $address server .truncate_from. ':'
  $port_no server .behind. ':'
  !port_no
    if
      port_no.is_empty
      -> 8080
      -> port_no.to_integer
  $model
    ai_types::model
      .name_of name
      .address_of address
      .port_no_of port_no
  get_tokens! model
    !model.ai::pieces_of
    !model.ai::begin_of_stream_token_of
    !model.ai::end_of_stream_token_of
    !model.ai::newline_token_of
    !model.ai::end_of_text_token_of
    !model.ai::prefix_token_of
    !model.ai::suffix_token_of
    !model.ai::middle_token_of
    !model.ai::context_size_of
  !model
    model
      .ai::question_token_of undefined
      .ai::answer_token_of undefined
  if
    ai::pieces_of(model).is_an_error
    -> ai::pieces_of(model)
    :
      build_piece_table &model

      #
	The default special tokens are only meant to be used with LLama models
	So let's replace them for other models (especially Falcon).

      !model.ai::newline_token_of ai::piece_table_of(model)("@nl;")

      for_each ai::pieces_of(model)
	: (token piece)
	  if
	    &&
	      piece.is_a_string
	      length_of(piece) > 4
	      ||
		piece .has_prefix. "<|" && piece .has_suffix. "|>"
		piece .has_prefix. ">>" && piece .has_suffix. "<<"
	    :
	      $special range(piece 3 -3)
	      case special
		"endoftext":
		  !model.ai::end_of_text_token_of token-1
		  next
		"QUESTION":
		  !model.ai::question_token_of token-1
		  next
		"ANSWER":
		  !model.ai::answer_token_of token-1
		  next
		"PREFIX":
		  !model.ai::prefix_token_of token-1
		  next
		"SUFFIX":
		  !model.ai::suffix_token_of token-1
		  next
		"MIDDLE":
		  !model.ai::middle_token_of token-1
		  next
		next
	    next
	-> model

$ai_types::model/ai::generate:
  (
    model
    options*
    prompt
  )
  extract_options options
    ai::MINIMUM_CONFIDENCE = 15 $minimum_confidence
    ai::MAXIMUM_LENGTH = 1024 $maximum_length
    ai::STOP = undefined $stop_text
    ai::BACKTRACK = 0 $backtrack
    ai::LOG_LEVEL = 0 $log_level
    ai::VERBOSE = false $be_verbose
    ai::USE_COLOURS = false $use_colours
  $do_log log_level > 0
  update_if use_colours &be_verbose -> true
  on use_colours: print! ansi_text_colour(BLACK)
  ai::tokenize! $tokens model prompt
  generate_text! $text $_max_no tokens "" 0 empty_list
  on use_colours: print! ansi_reset_colour()
  on be_verbose: println!
  if
    result_count() == 1
    -> text
    pass

  $generate_text: (tokens text no prefix)
    if
      stop_text.is_defined && text .contains. stop_text
      -> text no
      :
	current_time! $t1
	ai::evaluate! model tokens $next_token $confidence_values
	if
	  next_token == ai::end_of_stream_token_of(model):
	    on do_log:
	      eprintln! "<end of stream>"
	    -> text no
	  :
	    $max_no no
	    loop:
	      confidence_values(1) !next_token $next_confidence
	      ai::detokenize model next_token prefix $next_piece $suffix
	      on do_log:
		current_time! $t2
		$dt 1000*(t2-t1) # in ms
		if
		  log_level <= 1:
		    $token_string
		      "@quot;@(ai::escaped_piece(next_piece))@quot;:"
		    eprintln!
		      format
			"[#%4] %l18 %3.3"
			no token_string next_confidence
		  :
		    if
		      log_level >= 3:
			eprintln! format("-[#%4|%3.1 ms]---------------" no dt)
		      :
			eprintln! format("-[#%4]------------------------" no dt)
		    for_each confidence_values
		      : (confidence_value)
			confidence_value $token $confidence
			ai::detokenize model token prefix $piece
			replace_all &piece '@ht;' = "<<<ht>>>"
			$token_string "@quot;@(ai::escaped_piece(piece))@quot;:"
			eprintln!
			  format
			    "%5 %l18 %3.3" token token_string confidence
			next!
		      pass
	      if
		next_confidence >= minimum_confidence || no < 5:
		  update_if no == 0 &next_piece: trim_left next_piece
		  print_piece! next_piece next_confidence
		  if
		    no >= maximum_length
		    -> append(text next_piece) no
		    :
		      generate_text! $response $highest_no
			push(tokens next_token)
			append(text next_piece)
			no+1
			suffix
		      extend_to &max_no highest_no
		      if
			response.is_empty:
			  if
			    next_piece == "@nl;"
			    -> push(text '@nl;') max_no # stop backtracking
			    :
			      if
				||
				  length_of(confidence_values) < 2
				  max_no > no+backtrack
				-> "" max_no
				:
				  on use_colours: print! ansi_reset_colour()
				  on be_verbose: print!
				    dup("@bs; @bs;" length_of(next_piece))
				  on use_colours: print! ansi_text_colour(BLACK)
				  range &confidence_values 2 -1
				  next!
			-> response max_no
		-> "" max_no

  $print_piece: (piece confidence)
    on use_colours:
      $confidence_delta confidence-minimum_confidence
      $colour
	cond
	  -> confidence_delta < 0 ->
	    colour_mixture
	      BLACK = -confidence_delta
	      RED = minimum_confidence+confidence_delta
	  -> confidence_delta > 10 ->
	    colour_mixture
	      GREEN = 10
	      WHITE = confidence_delta-10
	  -> true ->
	    colour_mixture
	      RED = 10-confidence_delta
	      GREEN = confidence_delta
      print! ansi_background_colour(colour)
    on be_verbose: print! piece

$get_tokens: (model)
  #debug_write "

    ai_types::model/ai::get_tokens
  $json
    to_json
      insert_order_table
	"model" = name_of(model)
  open_tcp_socket! $fd address_of(model) port_no_of(model)
  print_to! fd "
    POST /get_tokens HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)
  load! $result fd
  #debug_write "
    @(result)
  $error_code between(result ' ' ' ').to_integer
  if
    error_code >= 300:
      error result .behind. "@cr;@nl;@cr;@nl;"
    :
      behind &result "@cr;@nl;@cr;@nl;"
      from_utf8 &result
      $info result.from_json
      ->
	info("tokens")
	info("begin_of_stream")
	info("end_of_stream")
	info("newline")
	info("end_of_text")
	info("prefix")
	info("suffix")
	info("middle")
	info("context_size")

$ai_types::model/ai::tokenize:
  #
    tokenizes the specified (list of) text(s)

    *text* can be a single text or a list of several text lines

    The result is a list of tokens or a list of list of tokens.
  (
    model # the model to query for tokens
    text # the text(s) to tokenize
  )
  #debug_write "

    ai_types::model/ai::tokenize
  $CONTENT
    if
      text.is_a_string
      -> "content"
      -> "lines"
  $json
    to_json
      insert_order_table
	"model" = name_of(model)
	CONTENT = text
  to_utf8 &json
  open_tcp_socket! $fd address_of(model) port_no_of(model)
  write_all_to! fd "
    POST /tokenize HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)
  load! $result fd
  #debug_write "
    @(result)
  behind &result "@cr;@nl;@cr;@nl;"
  $info result.from_json
  -> info("tokens")

$ai_types::model/ai::evaluate:
  #
    evaluate a prompt and return a token string or confidence values

    If the *prompt* is a string then the returned value is a the string
    corresponding to the generated token.

    If the prompt is a list of tokens then the returned value is a list of
    10 tuples <*token_id*, *confidence*>.

    Attention: In the case the prompt is a list of tokens a
    "*begin of stream*"-token will **not** be added automatically!
  (
    model # the model on which to evaluate the prompt
    prompt # the prompt to evaluate
  )
  #debug_write "

    ai_types::model/ai::evaluate
  $do_return_confidence_values result_count() == 2
  $use_tokens not(prompt.is_a_string)
  $PROMPT
    if
      use_tokens
      -> "tokens"
      -> "prompt"
  $json
    to_json
      insert_order_table
	PROMPT = prompt
	"model" = name_of(model)
	"logits" = true # also disables server side token selection
	"brief" = true
	"n_probs" = 10
	"n_predict" = 1
  to_utf8 &json
  open_tcp_socket! $fd address_of(model) port_no_of(model)
  write_all_to! fd "
    POST /completion HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)
  load! $result fd
  #debug_write "
    @(result)
  behind &result "@cr;@nl;@cr;@nl;"
  $info result.from_json
  if
    do_return_confidence_values:
      $logits info("logits")
      map &logits: (logit) tuple logit(1) logit(2)
      if
	use_tokens
	-> info("token") logits
	-> info("content") logits
    :
      if
	use_tokens
	-> info("token")
	-> info("content")

$ai_types::model/ai::detokenize:
  (
    model
    token_or_list
    prefix = empty_list
  )
  $rc result_count()
  $pieces ai::pieces_of(model)
  if
    token_or_list.is_a_list:
      $text ""
      for_each token_or_list
	: (token)
	  token_to_piece token $str &prefix
	  append &text str
	  next
	:
	  if
	    rc == 1
	    -> text
	    -> text prefix
    :
      token_to_piece token_or_list $str &prefix
      if
	rc == 1
	-> str
	-> str prefix

  $token_to_piece: (token prefix_octets)
    $piece pieces(token+1)
    if
      piece.is_a_list:
	append prefix_octets &piece
	$n length_of(piece)
	$i 1
	$e undefined
	loop:
	  update_if i-1 <= n &e -> i-1
	  if
	    i > n:
	      $octets ""
	      for_each range(piece 1 e)
		: (code)
		  push &octets character(code)
		  next
		-> from_utf8(octets) range(piece e+1 -1)
	    :
	      $code piece(i)
	      cond
		-> code < 0x80:
		  inc &i
		  next
		-> code & 0xe0 == 0xc0:
		  plus &i 2
		  next
		-> code & 0xf0 == 0xe0:
		  plus &i 3
		  next
		-> code & 0xf8 == 0xf0:
		  plus &i 4
		  next
		-> true -> "<???>" empty_list
      :
	update_if any_of(piece: (chr) -> chr == '@0x142;') &piece -> "<???>"
	  # 'ł' cannot be printed - why?
	-> piece empty_list

$build_piece_table: (model)
  $pieces ai::pieces_of(model)
  $piece_table empty_hash_table
  for_each pieces
    : (idx piece)
      if
	piece.is_a_string:
	  !piece_table(piece) idx-1
	  next
	next # ignore incomplete sequences
    ->
      model
	.ai::piece_table_of piece_table
	.ai::maximum_piece_length_of map_reduce(pieces length_of max)

$ai::escaped_token: (model token)
  -> ai::escaped_piece(ai::detokenize(model token))

$ai::escaped_piece: (piece)
  $buf ""
  $s 1
  $n length_of(piece)
  $i 0
  loop:
    inc &i
    if
      i > n
      -> append(buf range(piece s n))
      :
	$chr piece(i)
	case chr
	  '@nl;':
	    append &buf range(piece s i-1)
	    append &buf "\n"
	    !s i+1
	    next
	  '@quot;', '\':
	    append &buf range(piece s i-1)
	    push &buf '\'
	    push &buf chr
	    !s i+1
	    next
	  next
