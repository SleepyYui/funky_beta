#
  Copyright (C) 2023 by
  Dipl.-Ing. Michael Niederle

  This program is free software; you can redistribute it and/or modify
  it under the terms of the GNU Library General Public License, version 2, or
  (at your option) under the terms of the GNU Lesser General Public License,
  version 3.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
  GNU Lesser (Library) General Public License for more details.

  For details of the GNU General Public License see the accompanying
  files LGPLv2.txt and LGLPv3.txt or
  http://www.gnu.org/licenses/lgpl-2.0.html
  http://www.gnu.org/licenses/lgpl-3.0.html
  or print to the
  Free Software Foundation, Inc.,
  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.

<namespace ai>
<namespace ai_types>

<allow unused>

#
  Topic: AI

  Funky currently supports large language models based on the llama.cpp project.

# Events

$ai::AI_GOT_TOKENS .
$ai::AI_GET_TOKENS_ERROR .

# Modes

$ai::EXACT .
$ai::SHIFT .
$ai::SMART .

$DEFAULT_PORT_NO 8080

# Attributes

$ai::pieces_of ()
  #
    a list of pieces (token texts)

$ai::piece_table_of ()
  #
    a hash table that maps all valid pieces to tokens

$ai::maximum_piece_length_of ()
  #
    the length of the longest piece (token string)

$ai::begin_of_stream_token_of ()
  #
    the token no. used to denote the begin of a stream

$ai::end_of_stream_token_of ()
  #
    the token no. used to denote the end of a stream

$ai::newline_token_of ()
  #
    the token no. used to denote a newline character

$ai::space_token_of ()
  #
    the token no. used to denote a space character

$ai::end_of_text_token_of ()
  #
    the token no. used to denote the end of a text

$ai::prefix_token_of ()
  #
    the token no. used to denote a prefix

$ai::suffix_token_of ()
  #
    the token no. used to denote a suffix

$ai::middle_token_of ()
  #
    the token no. used to denote the "middle"

$ai::question_token_of ()

$ai::answer_token_of ()

$ai::context_size_of ()
  #
    the size of the context (number of tokens)

$std::address_of ()
  #
    the (IP) address of the server

$std::port_no_of ()
  #
    the port no. of the server

$uuid_of ()

# Methods

$ai::evaluate ()
  #
    evaluate a prompt string or list of tokens

$ai::tokenize ()
  #
    convert a string into a list of tokens

$ai::detokenize ()
  #
    convert a single token or a list of tokens into a string

$ai::generate ()
  #
    generate a response for the specified prompt

$ai::deregister ()
  #
    deregister from the server

# Options

$ai::SERVER_NAME .
  #
    the name of the server's executable

$ai::CONTEXT_SIZE .
  #
    the maxium size of the context (number of tokens)

$ai::MODEL_PATH .
  #
    the path where models are stored

$ai::MINIMUM_CONFIDENCE .
  #
    the minimum confidence level to continue generation

$ai::MAXIMUM_LENGTH .
  #
    the maximum number of tokens to generate

$ai::STOP .
  #
    a stop text

$ai::BACKTRACK .
  #
    specfies the maximum number of backtracking steps

$ai::LOG_LEVEL .
  #
    specifies the log level

$ai::VERBOSE .
  #
    if set the response text will be displayed during generation

$ai::USE_COLOURS .
  #
    use colours to display the response text

# Prototype Objects

$ai_types::connection std_types::object
  #
    a connection to an AI server

$ai_types::model std_types::object
  #
    the base object for all AI models

# Implementation

$ai::list_ai_models:
  #
    returns a list of available models
  (
     options* # connection options
  )
  extract_options options
    ADDRESS = "127.0.0.1" $address
    PORT_NO = DEFAULT_PORT_NO $port_no
    UUID = undefined $uuid
  $json
    to_json
      insert_order_table
	"uuid" = uuid
  to_utf8 &json
  open_tcp_client_socket! $fd address port_no
  write_to! fd "
    POST /list_models HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)
  load! $result fd
  on result.is_an_error result
  behind &result "@cr;@nl;@cr;@nl;"
  from_utf8 &result
  $info result.from_json
  -> info("models")

$ai::load_ai_model ()

$std_types::string/ai::load_ai_model:
  #
    load an AI model

    Topic: AI
  (
    name # the name of the model
    options* # connection options
  )
  initialize_model $model name options
  get_tokens! model
    !model.ai::pieces_of
    !model.ai::begin_of_stream_token_of
    !model.ai::end_of_stream_token_of
    !model.ai::newline_token_of
    !model.ai::end_of_text_token_of
    !model.ai::prefix_token_of
    !model.ai::suffix_token_of
    !model.ai::middle_token_of
    !model.ai::context_size_of
  !model
    model
      .ai::question_token_of undefined
      .ai::answer_token_of undefined
  if
    ai::pieces_of(model).is_an_error
    -> ai::pieces_of(model)
    :
      build_piece_table &model

      #
	The default special tokens are only meant to be used with LLama models
	So let's replace them for other models (especially Falcon).

      !model.ai::newline_token_of ai::piece_table_of(model)("@nl;")
      !model.ai::space_token_of ai::piece_table_of(model)(" ")

      for_each ai::pieces_of(model)
	: (token piece)
	  if
	    &&
	      piece.is_a_string
	      length_of(piece) > 4
	      ||
		piece .has_prefix. "<|" && piece .has_suffix. "|>"
		piece .has_prefix. ">>" && piece .has_suffix. "<<"
	    :
	      $special range(piece 3 -3)
	      case special
		"endoftext":
		  !model.ai::end_of_text_token_of token-1
		  next
		"QUESTION":
		  !model.ai::question_token_of token-1
		  next
		"ANSWER":
		  !model.ai::answer_token_of token-1
		  next
		"PREFIX":
		  !model.ai::prefix_token_of token-1
		  next
		"SUFFIX":
		  !model.ai::suffix_token_of token-1
		  next
		"MIDDLE":
		  !model.ai::middle_token_of token-1
		  next
		next
	    next
	-> model

$model_of ()

$std_types::io.model_of empty_hash_table

$std_types::io/ai::load_ai_model:
  #
    load an AI model

    Topic: AI
  (
    io # the input/output object
    id # the id to use for tagging the reply event
    name # the name of the model
    options* # connection options
  )
  update_if length_of(options) == 1 && options(1).is_a_list
    &options -> options(1)
  initialize_model $model name options
  tuple &id 1
  run &io http_request id
    address_of(model) port_no_of(model) get_tokens_request(model)
  !io.model_of(id) model
  register_handlers io id
    JOB_COMPLETED = load_ai_model_completed
    JOB_FAILED = load_ai_model_failed

$load_ai_model_completed: (io id data)
  $model model_of(io)(id)
  !io.model_of(id) undefined
  deregister_all_handlers &io id
  id !id
  from_utf8 &data
  from_json &data
  if
    data.is_an_error || data.is_undefined
    -> io tuple(JOB_FAILED id error(IO_ERROR "INVALID JSON"))
    :
      !model
	.ai::pieces_of data("tokens")
	.ai::begin_of_stream_token_of data("begin_of_stream")
	.ai::end_of_stream_token_of data("end_of_stream")
	.ai::newline_token_of data("newline")
	.ai::end_of_text_token_of data("end_of_text")
	.ai::prefix_token_of data("prefix")
	.ai::suffix_token_of data("suffix")
	.ai::middle_token_of data("middle")
	.ai::context_size_of data("context_size")
      -> io tuple(JOB_COMPLETED id model)

$load_ai_model_failed: (io id err)
  !io.model_of(id) undefined
  deregister_all_handlers &io id
  id !id
  -> io tuple(JOB_FAILED id err)

$initialize_model: (name options)
  extract_options options
    ADDRESS = "127.0.0.1" $address
    PORT_NO = DEFAULT_PORT_NO $port_no
    UUID = undefined $uuid
  ->
    ai_types::model
      .name_of name
      .address_of address
      .port_no_of port_no
      .uuid_of uuid

$ai_types::model/ai::generate:
  (
    model
    options*
    prompt
  )
  extract_options options
    ai::MINIMUM_CONFIDENCE = 15 $minimum_confidence
    ai::MAXIMUM_LENGTH = 1024 $maximum_length
    ai::STOP = undefined $stop_text
    ai::BACKTRACK = 0 $backtrack
    ai::LOG_LEVEL = 0 $log_level
    ai::VERBOSE = false $be_verbose
    ai::USE_COLOURS = false $use_colours
  $do_log log_level > 0
  update_if use_colours &be_verbose -> true
  on use_colours: print! ansi_text_colour(BLACK)
  ai::tokenize! $tokens model prompt
  generate_text! $text $_max_no tokens "" 0 empty_list
  on use_colours: print! ansi_reset_colour()
  on be_verbose: println!
  if
    result_count() == 1
    -> text
    pass

  $generate_text: (tokens text no prefix)
    if
      stop_text.is_defined && text .contains. stop_text
      -> text no
      :
	current_time! $t1
	ai::evaluate! model tokens $next_token $confidence_values
	if
	  next_token == ai::end_of_stream_token_of(model):
	    on do_log:
	      eprintln! "<end of stream>"
	    -> text no
	  :
	    $max_no no
	    loop:
	      confidence_values(1) !next_token $next_confidence
	      ai::detokenize model next_token prefix $next_piece $suffix
	      on do_log:
		current_time! $t2
		$dt 1000*(t2-t1) # in ms
		if
		  log_level <= 1:
		    $token_string
		      "@quot;@(ai::escaped_piece(next_piece))@quot;:"
		    eprintln!
		      format
			"[#%4] %l18 %3.3"
			no token_string next_confidence
		  :
		    if
		      log_level >= 3:
			eprintln! format("-[#%4|%3.1 ms]---------------" no dt)
		      :
			eprintln! format("-[#%4]------------------------" no dt)
		    for_each confidence_values
		      : (confidence_value)
			confidence_value $token $confidence
			ai::detokenize model token prefix $piece
			replace_all &piece '@ht;' = "<<<ht>>>"
			$token_string "@quot;@(ai::escaped_piece(piece))@quot;:"
			eprintln!
			  format
			    "%5 %l18 %3.3" token token_string confidence
			next!
		      pass
	      if
		next_confidence >= minimum_confidence || no < 5:
		  update_if no == 0 &next_piece: trim_left next_piece
		  print_piece! next_piece next_confidence
		  if
		    no >= maximum_length
		    -> append(text next_piece) no
		    :
		      generate_text! $response $highest_no
			push(tokens next_token)
			append(text next_piece)
			no+1
			suffix
		      max &max_no highest_no
		      if
			response.is_empty:
			  if
			    next_piece == "@nl;"
			    -> push(text '@nl;') max_no # stop backtracking
			    :
			      if
				||
				  length_of(confidence_values) < 2
				  max_no > no+backtrack
				-> "" max_no
				:
				  on use_colours: print! ansi_reset_colour()
				  on be_verbose: print!
				    dup("@bs; @bs;" length_of(next_piece))
				  on use_colours: print! ansi_text_colour(BLACK)
				  range &confidence_values 2 -1
				  next!
			-> response max_no
		-> "" max_no

  $print_piece: (piece confidence)
    on use_colours:
      $confidence_delta confidence-minimum_confidence
      $colour
	cond
	  -> confidence_delta < 0 ->
	    colour_mixture
	      BLACK = -confidence_delta
	      RED = minimum_confidence+confidence_delta
	  -> confidence_delta > 10 ->
	    colour_mixture
	      GREEN = 10
	      WHITE = confidence_delta-10
	  -> true ->
	    colour_mixture
	      RED = 10-confidence_delta
	      GREEN = confidence_delta
      print! ansi_background_colour(colour)
    on be_verbose: print! piece

$get_tokens: (model)
  $request get_tokens_request(model)
  open_tcp_client_socket! $fd address_of(model) port_no_of(model)
  print_to! fd request
  load! $result fd
  $error_code between(result ' ' ' ').to_integer
  if
    error_code >= 300:
      error result .behind. "@cr;@nl;@cr;@nl;"
    :
      behind &result "@cr;@nl;@cr;@nl;"
      from_utf8 &result
      $info result.from_json
      ->
	info("tokens")
	info("begin_of_stream")
	info("end_of_stream")
	info("newline")
	info("end_of_text")
	info("prefix")
	info("suffix")
	info("middle")
	info("context_size")

$get_tokens_request: (model)
  $json
    to_json
      insert_order_table
	"model" = name_of(model)
	"uuid" = uuid_of(model)
  to_utf8 &json
  -> "
    POST /get_tokens HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)

$ai_types::model/ai::tokenize:
  #
    tokenizes the specified (list of) text(s)

    *text* can be a single text or a list of several text lines

    The result is a list of tokens or a list of list of tokens.
  (
    model # the model to query for tokens
    text # the text(s) to tokenize
  )
  $request tokenize_request(model text)
  open_tcp_client_socket! $fd address_of(model) port_no_of(model)
  write_to! fd request
  load! $result fd
  behind &result "@cr;@nl;@cr;@nl;"
  from_utf8 &result
  $info result.from_json
  -> info("tokens")

$std_types::io/ai::tokenize:
  #
    tokenizes the specified (list of) text(s)

    *text* can be a single text or a list of several text lines

    The result is delivered via an AI_GOT_TOKENS event.
  (
    io # the IO object to handle the request
    id
    model # the model to query for tokens
    text # the text(s) to tokenize
  )
  tuple &id 1
  run &io http_request id
    address_of(model) port_no_of(model) tokenize_request(model text)
  register_handlers io id
    JOB_COMPLETED = tokenize_completed
    JOB_FAILED = tokenize_failed

$tokenize_completed: (io id data)
  deregister_all_handlers &io id
  id !id
  from_utf8 &data
  from_json &data
  if
    data.is_an_error || data.is_undefined
    -> io tuple(JOB_FAILED id error(IO_ERROR "INVALID JSON"))
    -> io tuple(JOB_COMPLETED id data("tokens"))

$tokenize_failed: (io id err)
  deregister_all_handlers &io id
  id !id
  -> io tuple(JOB_FAILED id err)

$tokenize_request: (model text)
  $CONTENT
    if
      text.is_a_string
      -> "content"
      -> "lines"
  $json
    to_json
      insert_order_table
	"model" = name_of(model)
	"uuid" = uuid_of(model)
	CONTENT = text
  to_utf8 &json
  -> "
    POST /tokenize HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)

$ai_types::model/ai::evaluate:
  #
    evaluate a prompt and return a token string or confidence values

    If the *prompt* is a string then the returned value is a the string
    corresponding to the generated token.

    If the prompt is a list of tokens then the returned value is a list of
    10 tuples <*token_id*, *confidence*>.

    Attention: In the case the prompt is a list of tokens a
    "*begin of stream*"-token will **not** be added automatically!
  (
    model # the model on which to evaluate the prompt
    prompt # the prompt to evaluate
    mode = ai::SMART # how to handle context change
    start = 0 # the number of fixed start tokens
  )
  $do_return_confidence_values result_count() == 2
  $use_tokens not(prompt.is_a_string)
  $request evaluate_request(model prompt mode start)
  open_tcp_client_socket! $fd address_of(model) port_no_of(model)
  write_to! fd request
  load! $result fd
  behind &result "@cr;@nl;@cr;@nl;"
  from_utf8 &result
  $info result.from_json
  if
    do_return_confidence_values:
      $logits info("logits")
      map &logits: (logit) tuple logit(1) logit(2)
      if
	use_tokens
	-> info("token") logits
	-> info("content") logits
    :
      if
	use_tokens
	-> info("token")
	-> info("content")

$std_types::io/ai::evaluate:
  #
    evaluate a prompt and return confidence values

    The prompt must be a list of tokens.

    The result is delivered via a JOB_COMPLETED event.
  (
    io # the IO object to handle the request
    id
    model # the model on which to evaluate the prompt
    prompt # the prompt to evaluate
    mode = ai::SMART # how to handle context change
  )
  tuple &id 1
  run &io http_request id
    address_of(model) port_no_of(model) evaluate_request(model prompt mode)
  register_handlers io id
    JOB_COMPLETED = evaluate_completed
    JOB_FAILED = evaluate_failed

$evaluate_completed: (io id data)
  deregister_all_handlers &io id
  id !id
  from_utf8 &data
  from_json &data
  if
    data.is_an_error || data.is_undefined
    -> io tuple(JOB_FAILED id error(IO_ERROR "INVALID JSON"))
    -> io tuple(JOB_COMPLETED id data("logits"))

$evaluate_failed: (io id err)
  deregister_all_handlers &io id
  id !id
  -> io tuple(JOB_FAILED id err)

$evaluate_request: (model prompt mode start = 0)
  $use_tokens not(prompt.is_a_string)
  $PROMPT
    if
      use_tokens
      -> "tokens"
      -> "prompt"
  case &mode
    ai::EXACT -> "exact"
    ai::SHIFT -> "shift"
    ai::SMART -> "smart"
    -> "exact"
  $json
    to_json
      insert_order_table
	PROMPT = prompt
	"start" = start
	"model" = name_of(model)
	"uuid" = uuid_of(model)
	"mode" = mode
	"logits" = true # also disables server side token selection
	"brief" = true
	"n_probs" = 10
	"n_predict" = 1
  to_utf8 &json
  -> "
    POST /completion HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)

$ai_types::model/ai::detokenize:
  (
    model
    token_or_list
    prefix = empty_list
  )
  $rc result_count()
  $pieces ai::pieces_of(model)
  if
    token_or_list.is_a_list:
      $text ""
      for_each token_or_list
	: (token)
	  token_to_piece token $str &prefix
	  append &text str
	  next
	:
	  if
	    rc == 1
	    -> text
	    -> text prefix
    :
      token_to_piece token_or_list $str &prefix
      if
	rc == 1
	-> str
	-> str prefix

  $token_to_piece: (token prefix_octets)
    $piece pieces(token+1)
    if
      piece.is_a_list:
	append prefix_octets &piece
	$n length_of(piece)
	$i 1
	$e undefined
	loop:
	  update_if i-1 <= n &e -> i-1
	  if
	    i > n:
	      $octets ""
	      for_each range(piece 1 e)
		: (code)
		  push &octets character(code)
		  next
		-> from_utf8(octets) range(piece e+1 -1)
	    :
	      $code piece(i)
	      cond
		-> code < 0x80:
		  inc &i
		  next
		-> code & 0xe0 == 0xc0:
		  plus &i 2
		  next
		-> code & 0xf0 == 0xe0:
		  plus &i 3
		  next
		-> code & 0xf8 == 0xf0:
		  plus &i 4
		  next
		-> true -> "<???>" empty_list
      :
	update_if any_of(piece: (chr) -> chr == '@0x142;') &piece -> "<???>"
	  # 'ł' cannot be printed - why?
	-> piece empty_list

$ai::connect_to_ai_server:
  #
    register at an AI server

    The server can be specified with the options

    <ADDRESS> and <PORT_NO>

    If the server is not specified then the default server is used.

    Topic: AI

    See also: ai::deregister
  (
    io
    id
    options* # connection options
  )
  update_if length_of(options) == 1 && options(1).is_a_list
    &options -> options(1)
  extract_options options
    ADDRESS = "127.0.0.1" $address
    PORT_NO = DEFAULT_PORT_NO $port_no
    UUID = undefined $uuid
  run io http_request id address port_no register_request(uuid)

$register_request: (uuid)
  $json
    to_json
      insert_order_table
	"uuid" = uuid
  to_utf8 &json
  -> "
    POST /register HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)

$std_types::string/ai::deregister:
  (
    uuid
    address = "127.0.0.1"
    port_no = DEFAULT_PORT_NO
  )
  sync_deregister! address port_no uuid

$ai_types::model/ai::deregister:
  (
    model
  )
  ai::deregister! uuid_of(model) "@(address_of(model)):@(port_no_of(model))"

$std_types::io/ai::deregister:
  (
    io
    id
    uuid_or_model
    address = "127.0.0.1"
    port_no = DEFAULT_PORT_NO
  )
  if
    uuid_or_model.is_a_string:
      async_deregister io id address port_no uuid_or_model
    :
      async_deregister io id
	address_of(uuid_or_model)
	port_no_of(uuid_or_model)
	uuid_of(uuid_or_model)

$sync_deregister: (address port_no uuid)
  open_tcp_client_socket! $fd address port_no
  write_to! fd deregister_request(uuid)
  load! $_result fd

$async_deregister: (io id address port_no uuid)
  run io http_request id address port_no deregister_request(uuid)

$deregister_request: (uuid)
  $json
    to_json
      insert_order_table
	"uuid" = uuid
  to_utf8 &json
  -> "
    POST /deregister HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)

$build_piece_table: (model)
  $pieces ai::pieces_of(model)
  $piece_table empty_hash_table
  for_each pieces
    : (idx piece)
      if
	piece.is_a_string:
	  if
	    piece_table(piece).is_defined
	    next
	    : # do not redefine a piece table entry
	      !piece_table(piece) idx-1
	      next
	next # ignore incomplete sequences
    ->
      model
	.ai::piece_table_of piece_table
	.ai::maximum_piece_length_of map_reduce(pieces length_of max)

$ai::escaped_token: (model token)
  -> ai::escaped_piece(ai::detokenize(model token))

$ai::escaped_piece: (piece)
  $buf ""
  $s 1
  $n length_of(piece)
  $i 0
  loop:
    inc &i
    if
      i > n
      -> append(buf range(piece s n))
      :
	$chr piece(i)
	case chr
	  '@nl;':
	    append &buf range(piece s i-1)
	    append &buf "\n"
	    !s i+1
	    next
	  '@quot;', '\':
	    append &buf range(piece s i-1)
	    push &buf '\'
	    push &buf chr
	    !s i+1
	    next
	  next

$ai::start_ai_server: (io id options*)
  flatten &options
  run io start_ai_server_action id options

$start_ai_server_action: (io id options)
  extract_options options
    ai::SERVER_NAME = "funky_server" $cmd
    UUID = undefined $uuid
    LOG_FILE = undefined $log_file
    ADDRESS = undefined $address
    PORT_NO = undefined $port_no
    ai::CONTEXT_SIZE = undefined $context_size
    ai::MODEL_PATH = undefined $model_path
  $server_options empty_list
  update_if uuid.is_defined &server_options:
    append server_options list("--uuid" uuid)
  update_if address.is_defined &server_options:
    append server_options list("--host" address)
  update_if port_no.is_defined &server_options:
    append server_options list("--port" port_no.to_string)
  update_if context_size.is_defined &server_options:
    append server_options list("--context-size" context_size.to_string)
  update_if model_path.is_defined &server_options:
    append server_options list("--model-path" model_path)
  if
    log_file.is_defined:
      open! $fd_log log_file "w+"
      open! $fd_null "/dev/null" "r"
      create_process! $pid cmd server_options undefined fd_null fd_log fd_log
      close! fd_null
      close! fd_log
      list_models!
    :
      open! $fd_null "/dev/null" "r"
      create_process! $pid cmd server_options undefined fd_null
      close! fd_null
      list_models!

  $list_models:
    log &io "list models"
    default_value &address "127.0.0.1"
    default_value &port_no DEFAULT_PORT_NO
    tuple &id 1
    register_handlers &io id
      JOB_COMPLETED = list_models_completed
      JOB_FAILED = list_models_failed
    run_at &io 2 http_request id address port_no list_models_request(uuid)
    -> io undefined

$list_models_completed: (io id data)
  deregister_all_handlers &io id
  id !id
  behind &data "@cr;@nl;@cr;@nl;"
  from_utf8 &data
  $info data.from_json
  -> io tuple(JOB_COMPLETED id info)

$list_models_failed: (io id err)
  debug::dump 50 `err
  deregister_all_handlers &io id
  id !id
  -> io tuple(JOB_FAILED id err)

$list_models_request: (uuid)
  $json
    to_json
      insert_order_table
	"uuid" = uuid
  to_utf8 &json
  -> "
    POST /list_models HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)
